{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLsensyrZsJS"
   },
   "source": [
    "# TP: Machine Learning (SIA_3611)\n",
    "\n",
    "## TP2: Classification (4h) \n",
    "\n",
    "by Clément Bouvier \n",
    "\n",
    "In machine learning, classification is related to supervised learning approaches in which the algorithm fits from an annotated set of data. This learning phase is followed by a validation phase to evaluate the classification model through several metrics. Once the model is correctly validated, a generalization phase is used to classify new data.\n",
    "\n",
    "The given dataset was produced by the World Health Organization. It pooled the evolution of 20 features for 15 years and among numerous countries. One of the goals of this TP2 is to visualize feature space and try to predict the development of countries.\n",
    "\n",
    "**Objectives:**\n",
    "- Visualize the feature space\n",
    "- Discuss the feasibility of feature space separation\n",
    "- Normalize the datasets\n",
    "- Train a K-NN, a decision tree, a random forest and a SVM\n",
    "- Visualize the decision boundary for each method\n",
    "- Create a test dataset\n",
    "- Compute AUC scores on a evaluation dataset\n",
    "- Tune hyperparameters\n",
    "- Visualize the modification of decision boundary for each tuning\n",
    "- Discuss the limits of the four implementations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5dSubbebJQ-"
   },
   "source": [
    "## STEP 1: Feature space visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbolZAMqOGzN"
   },
   "source": [
    "You will work on the WHO dataset in the year 2000. This first step consists of choosing two features to perform a classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvpYb9asOaCN"
   },
   "source": [
    "**TO DO 1.1**\n",
    "\n",
    "Execute the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDNpZKEOO85F"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"data/Life_Expectancy_Data.csv\")\n",
    "df = df.dropna()\n",
    "df.info()\n",
    "\n",
    "df1 = df[(df.Year == 2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzPeL_7xd5BC"
   },
   "outputs": [],
   "source": [
    "df_X = df1[['Total_expenditure', 'BMI']]\n",
    "df_Status = df1[['Status']]\n",
    "\n",
    "df_Y = df_Status.replace(['Developing', 'Developed'], [0, 1])\n",
    "\n",
    "np1 = df_X.to_numpy()\n",
    "plt.scatter(np1[:,0], np1[:,1], c=np.squeeze(df_Y.to_numpy()), cmap=matplotlib.colors.ListedColormap(['red', 'green']))\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WR1CNuNOTSgI"
   },
   "source": [
    "**QUESTION 1**\n",
    "\n",
    "Why was the label status binarized?\n",
    "\n",
    "Is this feature space easily separable? Justify your response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUmhMbD4TlBg"
   },
   "source": [
    "**TO CODE 1.2**\n",
    "\n",
    "Plot Total expenditure against Schooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SpfTKctFh3ro"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqvWF2ZOT6fV"
   },
   "source": [
    "**TO CODE 1.3**\n",
    "\n",
    "Plot Life Expectancy against Schooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pbz2jUJMh53f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1imRRQoUPer"
   },
   "source": [
    "**QUESTION 2**\n",
    "\n",
    "What would be the best features to use? Justify your response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8J_HeM0wNY93"
   },
   "source": [
    "## STEP 2: Dataset normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLIqbScfUyl1"
   },
   "source": [
    "To classify, the values in the learning dataset must be normalized (aka between 0 and 1). This normalization can be performed through various ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqxQ6NrnVNsE"
   },
   "source": [
    "**TO CODE 2.1**\n",
    "\n",
    "Normalize df_X. This normalization should perfectly frame the data (aka the minimum and the maximum values of each feature should be respectively 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CK613akYqrGD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ur1yR4XpX6xq"
   },
   "source": [
    "**TO DO 2.2**\n",
    "\n",
    "Each following cell performs a learning step and an AUC scores computation. For each classifier, several parameters have been chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tv_3eMSrDszE"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, plot_roc_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=5)\n",
    "clf1.fit(np_X_norm, np_Y)\n",
    "\n",
    "np_Y_pred = clf1.predict_proba(df_X_norm)\n",
    "\n",
    "print(roc_auc_score(np_Y, np_Y_pred[:,1]))\n",
    "plot_roc_curve(clf1, np_X_norm, np_Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bkJdVdMf-6Ay"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf2 = SVC(C=2.0, kernel='linear')\n",
    "clf2.fit(np_X_norm, np_Y)\n",
    "\n",
    "np_Y_pred = clf2.predict_proba(df_X_norm)\n",
    "\n",
    "print(roc_auc_score(np_Y, np_Y_pred[:,1]))\n",
    "plot_roc_curve(clf2, np_X_norm, np_Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBPKA7Sg-agW"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf3 = DecisionTreeClassifier(max_depth=3)\n",
    "clf3.fit(np_X_norm, np_Y)\n",
    "\n",
    "np_Y_pred = clf3.predict_proba(df_X_norm)\n",
    "\n",
    "print(roc_auc_score(np_Y, np_Y_pred[:,1]))\n",
    "plot_roc_curve(clf3, np_X_norm, np_Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nmhnBCUx_Q6X"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf4 = RandomForestClassifier(n_estimators=100, max_depth=3)\n",
    "clf4.fit(np_X_norm, np_Y)\n",
    "\n",
    "np_Y_pred = clf4.predict_proba(df_X_norm)\n",
    "\n",
    "print(roc_auc_score(np_Y, np_Y_pred[:,1]))\n",
    "plot_roc_curve(clf4, np_X_norm, np_Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKYdS0KNZSUQ"
   },
   "source": [
    "**QUESTION 3**\n",
    "\n",
    "Identify each used classifiers and specify the used parameters\n",
    "\n",
    "Describe and explain the results obtained for each ROC curve. What is the relation between the AUC and the ROC curve ? According to the ROC curve, which model is the best if we want to maximize the sensitivity ? And if we want to maximize the specificity ? Compare those results with the AUC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UlNGqBxZpiZ"
   },
   "source": [
    "For a two-classes problem, a decision boundary is a hypersurface which splits the feature space between two sets (for each class). Then this surface is composed of all the equiprobability points in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTclWyVibVDe"
   },
   "source": [
    "**TO DO 2.3**\n",
    "\n",
    "Plot the decision boundary with df_X for each classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeGb5-Oc6PxC"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "x_min, x_max = np_X_norm[:, 0].min() - 0.1, np_X_norm[:, 0].max() + 0.1\n",
    "y_min, y_max = np_X_norm[:, 1].min() - 0.1, np_X_norm[:, 1].max() + 0.1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, axarr = plt.subplots(2, 2, sharex='col', sharey='row', figsize=(10, 8))\n",
    "\n",
    "for idx, clf, tt in zip(product([0, 1], [0, 1]),\n",
    "                        [clf1, clf2, clf3, clf4],\n",
    "                        ['KNN', 'Linear SVM', 'Decision Tree', 'Random Forest']):\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    axarr[idx[0], idx[1]].contourf(xx, yy, Z, alpha=0.4)\n",
    "    axarr[idx[0], idx[1]].scatter(np_X_norm[:, 0], np_X_norm[:, 1], c=np_Y,\n",
    "                                  s=20, edgecolor='k')\n",
    "    axarr[idx[0], idx[1]].set_title(tt)\n",
    "\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N26rAHblbpMw"
   },
   "source": [
    "**QUESTION 4**\n",
    "\n",
    "What is the main problem of the classification step with this dataset?\n",
    "\n",
    "In your opinion, which classifier is better suited for this classification task? Justify your response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIan8C5HG1EC"
   },
   "source": [
    "## STEP 3: Biases correction and model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIy42WrbmKTB"
   },
   "source": [
    "In this part, you will focus on improving the AUC scores of the four methods.\n",
    "\n",
    "Firstable, you will weigh the classes to balance the classifier response. Then you will tune various hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWkUR2dFmk3e"
   },
   "source": [
    "**TO CODE 3.1**\n",
    "\n",
    "Compute the percentage of \"Developed\" class against the size the np_Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gk8uStv2l5cj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PErx0dejnKBY"
   },
   "source": [
    "SVM, Decision Tree and Random Forest algorithms have a parameter named:\n",
    "\n",
    "```\n",
    "class_weight\n",
    "```\n",
    "Here is a extract from sklearn documentation:\n",
    "\n",
    "**class_weight** dict, list of dict or “balanced”, default=None\n",
    "\n",
    "Weights associated with classes in the form {class_label: weight}. If None, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n",
    "\n",
    "The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_G6So0vnr0c"
   },
   "source": [
    "**QUESTION 5**\n",
    "\n",
    "What would be the weight values for each class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewqT7btZn07G"
   },
   "source": [
    "**TO CODE 3.2**\n",
    "\n",
    "Balance SVM, Decision Tree and Random Forest classifiers and plot the decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x33o_f4Pn_mW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbnKAcElsqI7"
   },
   "source": [
    "**TO CODE 3.3**\n",
    "\n",
    "Compute the new AUC scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dkpm3R9Dsvon"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qInghzh8sxzX"
   },
   "source": [
    "**QUESTION 6**\n",
    "\n",
    "Why did the AUC scores increase? How do you interpret it?\n",
    "\n",
    "In your opinion, has the classe balance improved the classification?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwQR6dM2t2oD"
   },
   "source": [
    "Let's focus on the SVM classifier.\n",
    "\n",
    "Sklearn allows multiple kernels.\n",
    "\n",
    "Here is an extract of the documentation:\n",
    "\n",
    "**kernel** {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’\n",
    "\n",
    "Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape (n_samples, n_samples).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLGU5Itaue9g"
   },
   "source": [
    "**TO CODE 3.4**\n",
    "\n",
    "Test the gaussian and the polynomial kernels with balanced classes and plot the decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rx4bVVjrEuzw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt7iX-ozuvOI"
   },
   "source": [
    "**QUESTION 7**\n",
    "\n",
    "In your opinion, which is the best kernel for this dataset? Justify your response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vN-CbU-u-dd"
   },
   "source": [
    "**BONUS**\n",
    "\n",
    "Tune the parameters for Decision Tree and Random Forest algorithms and plot the decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e81rsBAYGu_3"
   },
   "source": [
    "## STEP 4: An evaluation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "69LKeU_ivUKr"
   },
   "source": [
    "This part is to test the generalization of your models.\n",
    "\n",
    "You trained several classifiers on two features extracted from the year 2000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3Y2PtfYwAJ6"
   },
   "source": [
    "**TO CODE 4.1**\n",
    "\n",
    "Apply your models on the year 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xpn4Pto-GusP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F468vtVkynQA"
   },
   "source": [
    "**QUESTION 8**\n",
    "\n",
    "Are your models still relevant in the year 2012?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP2_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
