{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNC2ACmpftP0vPaVWo+olQv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MurphLaws/Exp/blob/main/SelectionCriteria.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRefRxOT-Mpy",
        "outputId": "afcbd359-efb1-4dc5-cb45-63646fb76617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.9.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install keras\n",
        "!pip install influenciae\n",
        "\n",
        "from math import ceil\n",
        "import cv2\n",
        "import time\n",
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from collections import namedtuple\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import math\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format='retina'\n",
        "\n",
        "\n",
        "\n",
        "from deel.influenciae.common import InfluenceModel, ConjugateGradientDescentIHVP, ExactIHVP\n",
        "from deel.influenciae.influence import FirstOrderInfluenceCalculator\n",
        "from deel.influenciae.utils import ORDER\n",
        "from deel.influenciae.plots import plot_datacentric_explanations\n",
        "\n",
        "\n",
        "print(tf.__version__)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class Dataset:\n",
        "\n",
        "    def __init__(self, train = pd.DataFrame(), test = pd.DataFrame(), df = None):\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "        self.df = pd.concat([train,test]).sort_index()\n",
        "\n",
        "\n",
        "    def populate(self, cluster_number,n,df=None, variance = 0.1):\n",
        "        random.seed(2349324)\n",
        "        np.random.seed(2349324)\n",
        "        tf.random.set_seed(2349324)\n",
        "\n",
        "        if cluster_number == 2:\n",
        "            covmat = [[variance, 0], [0, variance]]\n",
        "            mean_1 = [0,0]\n",
        "            class1_x1, class1_x2 = np.random.multivariate_normal(mean_1, covmat, int(n/2)).T\n",
        "\n",
        "            mean_2 = [2,2]\n",
        "            class2_x1, class2_x2 = np.random.multivariate_normal(mean_2, covmat, int(n/2)).T\n",
        "\n",
        "            part1 = list(zip(class1_x1, class1_x2,itertools.repeat(0)))\n",
        "            part2 = list(zip(class2_x1, class2_x2,itertools.repeat(1)))\n",
        "\n",
        "            self.df = pd.DataFrame(np.concatenate((part1,part2),axis=0), columns = [\"X1\", \"X2\", \"Label\"])\n",
        "\n",
        "        if cluster_number == 3:\n",
        "            covmat = [[variance, 0], [0, variance]]\n",
        "\n",
        "\n",
        "            len1 = int(n/3)\n",
        "            len2 = int(n/3)\n",
        "            len3 = n-len1-len2\n",
        "\n",
        "            mean_1 = [0,0]\n",
        "            class1_x1, class1_x2 = np.random.multivariate_normal(mean_1, covmat, len1).T\n",
        "\n",
        "            mean_2 = [4,0]\n",
        "            class2_x1, class2_x2 = np.random.multivariate_normal(mean_2, covmat, len2).T\n",
        "\n",
        "            mean_3 = [2,2*math.sqrt(3)]\n",
        "            class3_x1, class3_x2 = np.random.multivariate_normal(mean_3, covmat, len3).T\n",
        "\n",
        "            part1 = list(zip(class1_x1, class1_x2,itertools.repeat(0)))\n",
        "            part2 = list(zip(class2_x1, class2_x2,itertools.repeat(2)))\n",
        "            part3 = list(zip(class3_x1, class3_x2,itertools.repeat(1)))\n",
        "\n",
        "            self.df = pd.DataFrame(np.concatenate((part1,part2,part3),axis=0), columns = [\"X1\", \"X2\", \"Label\"])\n",
        "        self.train = self.df.sample(frac=0.8, random_state=1).copy()\n",
        "        self.test = self.df.drop(self.train.index).copy()\n",
        "\n",
        "    def get_all_data(self):\n",
        "        return self.df\n",
        "\n",
        "    def plot_data(self):\n",
        "        plt.figure(figsize=(8,7))\n",
        "\n",
        "\n",
        "        sns.scatterplot(data=self.df, x='X1', y='X2', hue='Label', palette='viridis', style='Label',s=100, linewidth=0)\n",
        "        plt.xlabel('X1')\n",
        "        plt.ylabel('X2')\n",
        "        plt.legend()\n",
        "        plt.title(\"Data Scatter Plot\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def train_predict(self, output = 'loss'):\n",
        "\n",
        "        x_train = self.train[['X1','X2']]\n",
        "        y_train = self.train['Label']\n",
        "\n",
        "        x_test  = self.test[['X1','X2']]\n",
        "        y_test  = self.test['Label']\n",
        "\n",
        "        np.random.seed(2349324)\n",
        "        fan_avg = (x_train.shape[1] + 1) / 2  # Average of input and output dimensions\n",
        "        limit = np.sqrt(6 / fan_avg)\n",
        "        weights_init = np.random.uniform(low=-limit, high=limit, size=(x_train.shape[1],))\n",
        "\n",
        "        logreg = LogisticRegression(solver='liblinear', random_state=2349324, verbose=0)\n",
        "        logreg.coef_ = weights_init.reshape(1, -1)\n",
        "\n",
        "        logreg.fit(x_train, y_train)\n",
        "\n",
        "        y_pred  = logreg.predict(x_test)\n",
        "        y_proba = logreg.predict_proba(x_test)\n",
        "\n",
        "        if output == 'loss':\n",
        "            return np.array(Dataset.binary_cross_entropy(list(y_test), y_proba))\n",
        "        elif output == 'boundary':\n",
        "            return logreg\n",
        "        elif output == 'proba':\n",
        "            return y_pred, y_proba\n",
        "\n",
        "    def plot_boundary(self):\n",
        "        _, ax = plt.subplots(figsize=(8, 7))\n",
        "        DecisionBoundaryDisplay.from_estimator(\n",
        "            self.train_predict(output='boundary'),\n",
        "            self.train[['X1','X2']],\n",
        "            ax=ax,\n",
        "            response_method=\"predict\",\n",
        "            plot_method=\"pcolormesh\",\n",
        "            xlabel=\"X1\",\n",
        "            ylabel=\"X2\",\n",
        "            alpha = 0.3\n",
        "        )\n",
        "\n",
        "        sns.scatterplot(data=self.train, x='X1', y='X2', hue='Label', palette='viridis', style='Label',s=100, linewidth=0)\n",
        "        plt.title('Data Visualization')\n",
        "        plt.show()\n",
        "\n",
        "    def loo(self,groups):\n",
        "        datasets = []\n",
        "        for group in groups:\n",
        "            datasets.append((group,Dataset(self.train.drop(group),self.test)))\n",
        "        return datasets\n",
        "\n",
        "    @staticmethod\n",
        "    def binary_cross_entropy(predictions, probabilities):\n",
        "        binary_crossentropy = []\n",
        "        for i in range(len(predictions)):\n",
        "            b = predictions[i]\n",
        "            p = probabilities[i][1]  # Assuming the probability of the positive class is at index 1\n",
        "\n",
        "            # Calculate the binary crossentropy\n",
        "            cross_entropy = -b * np.log(p) - (1 - b) * np.log(1 - p)\n",
        "            binary_crossentropy.append(cross_entropy)\n",
        "\n",
        "        return binary_crossentropy\n",
        "\n",
        "\n",
        "\n",
        "    def get_inf_matrix(self, groups):\n",
        "        datasets = self.loo(groups)\n",
        "        inf_matrix = pd.DataFrame(columns=list(datasets[0][1].test.index))#0, index=groups, columns=list(datasets[0][1].test.index))\n",
        "\n",
        "        for ds in tqdm(datasets):\n",
        "            inf_matrix.loc[str(ds[0])] = ds[1].train_predict()-self.train_predict()\n",
        "        return inf_matrix\n",
        "\n",
        "\n",
        "    def contaminate(self,position,n,label,variance=0.0005):\n",
        "        cov = [[variance,0],[0,variance]]\n",
        "        x1, x2= np.random.multivariate_normal(position, cov, n).T\n",
        "\n",
        "        final_index = self.df.index.max()\n",
        "        result = pd.DataFrame(list(zip(x1, x2,itertools.repeat(label))), columns = [\"X1\", \"X2\", \"Label\"])\n",
        "        reindexed_result = result.set_index(pd.RangeIndex(start=final_index + 1, stop=final_index + 1 + len(result)))\n",
        "        return (position, list(reindexed_result.index),Dataset(train=pd.concat([self.train.copy(),reindexed_result]),test=self.test))\n",
        "\n",
        "\n",
        "    def visualize_influence(self, matrix, sign,center,original):\n",
        "        #test_df = self.test.loc[list(matrix.columns)].lt(0)\n",
        "\n",
        "\n",
        "\n",
        "        group = self.train.loc[eval(list(matrix.index)[0])]\n",
        "\n",
        "        if sign == \"Negative\":\n",
        "            test_points = self.test.loc[list(matrix.where(matrix.lt(0)).dropna(axis=1).columns)].copy()\n",
        "            test_points[\"Influence\"]=matrix.where(matrix.lt(0)).dropna(axis=1).values.T\n",
        "\n",
        "        if sign == \"Positive\":\n",
        "            test_points = self.test.loc[list(matrix.where(matrix.gt(0)).dropna(axis=1).columns)].copy()\n",
        "            test_points[\"Influence\"]=matrix.where(matrix.gt(0)).dropna(axis=1).values.T\n",
        "\n",
        "        if sign == 'Marginal':\n",
        "            test_points = self.test.loc[list(matrix.columns)].copy()\n",
        "            test_points[\"Influence\"]=matrix.values.T\n",
        "\n",
        "\n",
        "        _, ax = plt.subplots(figsize=(8, 7))\n",
        "        DecisionBoundaryDisplay.from_estimator(\n",
        "        self.train_predict(output='boundary'),\n",
        "        self.train[['X1','X2']],\n",
        "        ax=ax,\n",
        "        response_method=\"predict\",\n",
        "        plot_method=\"pcolormesh\",\n",
        "        xlabel=\"X1\",\n",
        "        ylabel=\"X2\",\n",
        "        eps = 2,\n",
        "        alpha = 0.2\n",
        "        )\n",
        "\n",
        "\n",
        "        marker_map = {1:'X',0:'o',2:'s'}\n",
        "        center_df = pd.DataFrame({'x': [center[0]], 'y': [center[1]]})\n",
        "\n",
        "        sns.scatterplot(data=self.test, x='X1', y='X2',color='gray', style='Label',s=100, linewidth=0,alpha=0.3)\n",
        "        sp = sns.scatterplot(data=test_points, x='X1', y='X2', hue='Influence', palette='viridis', style='Label',s=100, linewidth=0, markers = {1:'X',0:'o',2:'s'})\n",
        "        sns.scatterplot(data=center_df, x='x', y='y',marker=marker_map[list(group['Label'])[0]], color='red', s=300)\n",
        "        #plt.scatter(group['X1'], group['X2'], s=100, c=\"black\",linewidths=1,edgecolors='black')\n",
        "        plt.xlabel('X1')\n",
        "        plt.ylabel('X2')\n",
        "\n",
        "\n",
        "        custom_handles = [\n",
        "        plt.scatter([], [], marker='o', s=80, color='black', label='0'),\n",
        "        plt.scatter([], [], marker='x', s=80, color='black', label='1'),\n",
        "        ]\n",
        "\n",
        "        norm = plt.Normalize(test_points['Influence'].min(), test_points['Influence'].max())\n",
        "        sm = plt.cm.ScalarMappable(cmap=\"viridis\", norm=norm)\n",
        "        sp.figure.colorbar(sm, ax=sp).set_label('Influence')\n",
        "\n",
        "        plt.legend(handles=custom_handles, title='Label')\n",
        "        plt.title(sign + \" influence on Test Set, n = \" + str(len(eval(matrix.index[0]))))\n",
        "        plt.xlim(-3,5)\n",
        "        plt.ylim(-3,5)\n",
        "        plt.grid(True, linewidth=1, linestyle='--', color='gray')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "centers_x = np.linspace(-1,3,5)\n",
        "centers_y = np.linspace(-1,3,5)\n",
        "points = list(itertools.product(centers_x, centers_y))\n",
        "\n",
        "no_1 = Dataset()\n",
        "no_1.populate(3, 5000, variance = 0.1)\n",
        "\n",
        "def run(means):\n",
        "    for m in means:\n",
        "        center, contaminated_group, contaminated_data = no_1.contaminate(m,10,1,variance = 0.005)\n",
        "        contaminated_data.plot_data()\n",
        "        matrix_1 = contaminated_data.get_inf_matrix([contaminated_group])\n",
        "        #contaminated_data.visualize_influence(matrix_1, \"Negative\",center, no_1)\n",
        "        contaminated_data.visualize_influence(matrix_1, \"Positive\",center, no_1)\n",
        "\n"
      ],
      "metadata": {
        "id": "9W5_Zxwe-V5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "synthData = Dataset()\n",
        "synthData.populate(2,1000, variance = 0.1)"
      ],
      "metadata": {
        "id": "dA5rj90Z-agd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}